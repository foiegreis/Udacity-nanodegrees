{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9e67d2",
   "metadata": {},
   "source": [
    "# Writeup: Track 3D-Objects Over Time - Midterm Project\n",
    "\n",
    "\n",
    "This is the Mid-Term Project for the second course in the [Udacity Self-Driving Car Engineer Nanodegree Program](https://www.udacity.com/course/c-plus-plus-nanodegree--nd213) : Sensor Fusion and Tracking.\n",
    "\n",
    "In this project, real-world data from [Waymo Open Dataset](https://console.cloud.google.com/storage/browser/waymo_open_dataset_v_1_2_0_individual_files) and 3D Point Cloud are used for LiDAR based Object Detection.\n",
    "\n",
    "\n",
    "## Project Sections\n",
    "\n",
    "The project is devided in 4 sections:\n",
    "\n",
    "1. Compute Lidar Point-Cloud from Range Image \n",
    "-  'ID_S1_EX1': Visualize range image channels\n",
    "-  'ID_S1_EX2': Visualize lidar point-cloud\n",
    "\n",
    "2. Create Birds-Eye View from Lidar PCL \n",
    "- 'ID_S2_EX1': Convert sensor coordinates to BEV-map coordinates\n",
    "- 'ID_S2_EX2': Compute intensity layer of the BEV map\n",
    "- 'ID_S2_EX3': Compute height layer of the BEV map\n",
    "\n",
    "3. Model-based Object Detection in BEV Image \n",
    "- 'ID_S3_EX1': Add a second model from a GitHub repo\n",
    "- 'ID_S3_EX2': Extract 3D bounding boxes from model response\n",
    "\n",
    "4. Performance Evaluation for Object Detection \n",
    "- 'ID_S4_EX1': Compute intersection-over-union between labels and detections\n",
    "- 'ID_S4_EX2': Compute false-negatives and false-positives\n",
    "- 'ID_S4_EX3': Compute precision and recall\n",
    "\n",
    "\n",
    "To run the project:\n",
    "\n",
    "1. refer to the README.md file for all the requirements (libraries, dataset)\n",
    "\n",
    "\n",
    "2. run the `loop_over_dataset.py` as follows:\n",
    "\n",
    "```\n",
    "python3 loop_over_dataset.py\n",
    "```\n",
    "In the loop_over_dataset.py you can select the ID_EX sections separately by selecting them in line 87\n",
    "\n",
    "All corresponding code for this project can be found in the `student` directory.\n",
    "The project has been run locally on a 2021 M1 MacbookPro \n",
    "\n",
    "\n",
    "\n",
    "## Project recap and analysis\n",
    "\n",
    "Let's go through the project sections with a closer look at:\n",
    "\n",
    "- Finding and displaying 10 examples of vehicles with varying degrees of visibility in the point-cloud\n",
    "- Identifying vehicle features that appear as a stable feature on most vehicles (e.g. rear-bumper, tail-lights) and describe them briefly. Also, use the range image viewer from the last example to underpin your findings using the lidar intensity channel.\n",
    "\n",
    "### Section 1. Compute Lidar Point-Cloud from Range Image \n",
    "\n",
    "This section includes the following steps in order to visualize the range-intensity images and the 3Dpoint clouds:\n",
    "-  'ID_S1_EX1': Visualize range image channels\n",
    "-  'ID_S1_EX2': Visualize lidar point-cloud\n",
    "\n",
    "We start retrieving the lidar data and range images of the roof-mounted lidar from the dataset and converting two channels (range and intensity) to 8bit scale and normalize the intensity channel between its 1-99 percentile in order to discard outliers. Then we stack the range and intensity channels vertically to visualize the range image.\n",
    "\n",
    "<img src=\"img/writeup-midterm/range_image.png\"/>\n",
    "\n",
    "Then we visualize the Lidar point cloud using the open3d library\n",
    "This will be the starter prespective\n",
    "\n",
    "<img src=\"img/writeup-midterm/upperview-pcd.png\"/>\n",
    "\n",
    "\n",
    "I've modified the loop_over_dataset.py and object_pcl.py in the ID_S1_EX1 part in order to display both the complete, the FRONT and the LEFT side range-intensity images.\n",
    "By zooming the point cloud object we can find the corresponding point areas in order to compare the two visualizations as below:\n",
    "\n",
    "<img src=\"img/writeup-midterm/range_image_total.png\"/>\n",
    "<img src=\"img/writeup-midterm/pcd_front.png\"/>\n",
    "<img src=\"img/writeup-midterm/pcd_left.png\"/>\n",
    "\n",
    "\n",
    "Looking at the range-intensity images, it's clear how the intensity channel is very sensitive about reflective vehicle parts such as licence plates, tail lights and front lights. It also discriminates well road line marks.\n",
    "\n",
    "<img src=\"img/writeup-midterm/range_image_boxes.png\"/>\n",
    "\n",
    "On the other side, 3D point clouds take account of the 3D shapes of the objects and their peculiar traits. In the below images we can assess how well windshields, wheels, side mirrors and the general vehicle shape is detected by the 3D point cloud \n",
    "\n",
    "<img src=\"img/writeup-midterm/windshields.png\"/>\n",
    "<img src=\"img/writeup-midterm/windshields_and_wheels.png\"/>\n",
    "\n",
    "As seen during the course, an important trait of the point cloud regards how Multiple Signal Returns\n",
    "are managed. For the sake of the course, the ri_return2 data from the waymo dataset are not used, but it would be very interesting to analyze how the point cloud empty areas will change using these data. \n",
    "\n",
    "\n",
    "### Section 2. Create Birds-Eye View from Lidar PCL \n",
    "\n",
    "This section includes the following steps in order to create and visualize the Bird-eye view (BEV) map:\n",
    "- 'ID_S2_EX1': Convert sensor coordinates to BEV-map coordinates\n",
    "- 'ID_S2_EX2': Compute intensity layer of the BEV map\n",
    "- 'ID_S2_EX3': Compute height layer of the BEV map\n",
    "\n",
    "In order to perform object detection, we consider projection-based approaches to reduce the dimensionality of the 3D point cloud along a specified dimension. One of the most used representations is the BEV map (bird's eye view map)(top-down view), a high information 2D projection of the 3D point cloud for behavior prediction and planning tasks. \n",
    "\n",
    "The BEV map pros are the following:\n",
    "- The objects of interest are located on the same plane as the sensor-equipped vehicle with only little variance\n",
    "- The BEV projection preserves the physical size and the proximity relations between objects, separating them more clearly than with both the FV and the RV projection.\n",
    "\n",
    "which are achieved by compacting the point cloud along the upward-facing axis (the zz-axis in the Waymo vehicle coordinate system). The BEV is divided into a grid consisting of equally sized cells, which enables us to treat it as an image, where each pixel corresponds to a region on the road surface.  [Source: course notes]\n",
    "\n",
    "Below: the BEV map from a frame in our dataset\n",
    "\n",
    "<img src=\"img/writeup-midterm/bev_map.png\"/>\n",
    "\n",
    "Next, we want to to fill the \"intensity\" channel of the BEV map with data from the point-cloud. In order to do so, we must identify all points with the same (x,y)-coordinates within the BEV map and then assign the intensity value of the top-most lidar point to the respective BEV pixel. Also, we must normalize the resulting intensity image using percentiles, in order to make sure that the influence of outlier values (very bright and very dark regions) is sufficiently mitigated and objects of interest (e.g. vehicles) are clearly separated from the background.\n",
    "\n",
    "Below: the intensity layer of the bev map\n",
    "\n",
    "<img src=\"img/writeup-midterm/bev_intensity.png\"/>\n",
    "\n",
    "As we can see, it's not easy to visualize the point cloud here. We'll need to add the \"height\" channel of the BEV map with data from the point-cloud as below:\n",
    "\n",
    "<img src=\"img/writeup-midterm/bev_map_final.png\"/>\n",
    "\n",
    "### Section 3. Model-based Object Detection in BEV Image\n",
    "\n",
    "Now, within the BEV map we can draw the bboxes of the objects from the ground truth labels which are present in the dataset. \n",
    "\n",
    "Before the detections can move along in the processing pipeline, they need to be converted into metric coordinates in vehicle space. This task is about performing this conversion such that all detections have the format [1, x, y, z, h, w, l, yaw], where 1 denotes the class id for the object type vehicle.\n",
    "\n",
    "\n",
    "Next, we can actually detect objects by loading the model from the Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (https://github.com/maudzung/SFA3D)\n",
    "\n",
    "The network has the following architecture:\n",
    "\n",
    "- ResNet-based Keypoint Feature Pyramid Network (KFPN) \n",
    "\n",
    "- Input: birds-eye-view (BEV) map as input. The BEV map is encoded by height, intensity, and density of 3D LiDAR point clouds. Assume that the size of the BEV input is (H, W, 3).\n",
    "\n",
    "- Outputs: Heatmap for main center with a size of (H/S, W/S, C) where S=4 (the down-sample ratio), and C=3 (the number of classes) \n",
    "\n",
    "- Objects: Cars, Pedestrians, Cyclists, but we'll perform the detection for the Cars/Vehicles class\n",
    "\n",
    "Below: the detection results on the SFA3D pretrained model\n",
    "\n",
    "<img src=\"img/writeup-midterm/detected.png\"/>\n",
    "\n",
    "### Section 4. Performance Evaluation for Object Detection \n",
    "\n",
    "In this section we finally assess the performances of the vehicles detection on the dataset.\n",
    "\n",
    "In order to do so, we perform the following steps:\n",
    "- 'ID_S4_EX1': Compute intersection-over-union between labels and detections\n",
    "- 'ID_S4_EX2': Compute false-negatives and false-positives\n",
    "- 'ID_S4_EX3': Compute precision and recall\n",
    "\n",
    "The charts below represent our results:\n",
    "\n",
    "**Please note**  we weren't able to visualize the plots using the terminal because of some system limitations (macbook m1). In order to display it, we run `loop_over_dataset.py` setting `ex=ID_S4_EX3` at line 87. The script will create a `data.json` file , in which the python dictionary datastructure containing our metrics results has been written. Then, by running the script `display_charts.py` from a Python IDE we'll be able to visualize the plots as below.\n",
    "\n",
    "<img src=\"img/writeup-midterm/performance.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e3752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cc3c926",
   "metadata": {},
   "source": [
    "# Writeup: Sensor Fusion and Object Tracking - Final Project\n",
    "\n",
    "This is the Final-Term Project for the second course in the [Udacity Self-Driving Car Engineer Nanodegree Program](https://www.udacity.com/course/c-plus-plus-nanodegree--nd213) : Sensor Fusion and Tracking.\n",
    "\n",
    "The final project is built over the Mid-Term project code and consists of four main steps:\n",
    "\n",
    "<img src=\"img/writeup-final/track.png\"/>\n",
    "\n",
    "\n",
    "## Project Sections\n",
    "\n",
    "Step 1: Implement EKF to track a single real-world target with lidar\n",
    "'F_ID_S1'\n",
    "\n",
    "Step 2: Implement the track management to initialize and delete tracks, set a track state and a track score\n",
    "'F_ID_S2'\n",
    "\n",
    "Step 3: Implement a single nearest neighbor data association to associate measurements to tracks (multitarget tracking)\n",
    "'F_ID_S3'\n",
    "\n",
    "Step 4: Implement the nonlinear camera measurement model\n",
    "'F_ID_S4'\n",
    "\n",
    "\n",
    "To run the project:\n",
    "\n",
    "1. refer to the README.md file for all the requirements (libraries, dataset)\n",
    "\n",
    "\n",
    "2. run the `loop_over_dataset.py` as follows: (pythonw from macbook terminal)\n",
    "\n",
    "```\n",
    "pythonw loop_over_dataset.py\n",
    "```\n",
    "In the loop_over_dataset.py you can select the F_ID sections separately by selecting them in line 103\n",
    "\n",
    "All corresponding code for this project can be found in the `student` directory.\n",
    "The project has been run locally on a 2021 M1 MacbookPro \n",
    "\n",
    "## Project recap and analysis\n",
    "\n",
    "Let's walk through the project steps\n",
    "\n",
    "### Step 1. Implement EKF to track a single real-world target with lidar\n",
    "\n",
    "This task involves writing code within the file `student/filter.py` in order to implement an Extended Kalman Filter to track the objects detected from our lidar sensor.\n",
    "\n",
    "The EKF algorithm involves the definition of the tracking problem in the state-space form as follows:\n",
    "\n",
    "- 1. The problem is defined by a linear model, with the state vector x = (px, py, pz, vx, vy, vz). Thus, the System Matrix F will take dimension (6, 6). \n",
    "\n",
    "- 2. The vehicles are modeled with constant-velocity \n",
    "\n",
    "- 3. Process noise Q, raising from this simplified modeling of the vehicles dynamics, is supposed to be random with mean=0 and covariance matrix Q. The bigger the entries in q, the more acceleration and braking we expect in the motion scenario. Here, by default, `params.q = 3 m/s^2` which defines a normal highway motion scenrio.\n",
    "\n",
    "- 4. Measurement noise is supposed to have mean 0 and covariance matrix R. The values of R depends on the sensor calibration and here are `params.sigma_lidar = 0.1` for Lidar sensor and `params.cam = 5` for Camera sensor.\n",
    "\n",
    "- 5. The tracking results will be evaluated via RMSE error\n",
    "\n",
    "\n",
    "In this step we'll define the `update()` and `predict()` functions in order to complete the Kalman filter.\n",
    "\n",
    "We'll run the filter on a single track, and these are the RMSE results for the tracking (note that the plot has been saved after updating the standard x and P values with the calculated ones)\n",
    "\n",
    "As you can see from the plot, the RMSE is <0.32 as expected.\n",
    "\n",
    "<img src=\"img/writeup-final/step1-rmse.png\"/>\n",
    "\n",
    "\n",
    "### Step 2: Implement the track management to initialize and delete tracks, set a track state and a track score\n",
    "\n",
    "This second task involves the completion of some functions inside the `student/trackmanagement.py` file.\n",
    "\n",
    "The track management system allows the EKF to perform tracking of multiple objects simultaneously, handling the current tracks, the vanishing ones and the new tracks in an effective way.\n",
    "\n",
    "To each tracked objects, a track ID, a tracking score and a tracking state will be assigned at each frame. Tracking states can get the values 'initialized', 'tentative' and 'confirmed'. \n",
    "\n",
    "After initializing a track, the track- measurement pairing process will determine its score and its state. Tracks with low scores, high uncertainty or no further measurements will be likely to be removed.\n",
    "\n",
    "The track management system will also be able to deal with False Positive trackings (clutters or Ghost Tracks) and False Negative trackings (occlusions)\n",
    "\n",
    "Below, the RMSE plot of a single track which has been deleted from the tracking list after some frames:\n",
    "\n",
    "<img src=\"img/writeup-final/step2-rmse.png\"/>\n",
    "\n",
    "### Step 3: Implement a single nearest neighbor data association to associate measurements to tracks (multitarget tracking)\n",
    "\n",
    "Data association is a key step of the EKF process, consisting in applying a specific algorithm in order to assign all each measurement to the track which is more likely to represent the object measured.\n",
    "\n",
    "In this first commit, we'll use the SNN (Simple Nearest Neighbor) algorithm to perform the association. Note that this algorithm while being simple is more prone to reach partial optima. More sophisticated algorithms like GNN and JPDA will be tested in the Improvement section. \n",
    "\n",
    "The SNN applies the standard Mahalanobis distance to each track-measurement pair, thus populating the Association matrix A. Gating will also be performed in order to reduce the computational cost.\n",
    "\n",
    "You'll find this step's code in the `student/association.py` file.\n",
    "\n",
    "Below, the RMSE plot of this multi-track step, which correctly tracks 3 confirmed objects \n",
    "\n",
    "<img src=\"img/writeup-final/step3-rmse.png\"/>\n",
    "\n",
    "\n",
    "### Step 4: Implement the nonlinear camera measurement model\n",
    "\n",
    "So far, we've been referring only on the Lidar sensor data for our tracking process. Thus, we'll now introduce the Camera data and see if there are some improvements over the same dataset.\n",
    "\n",
    "The Camera Model is must take care of the non linearity of the measurements function, which raises from the need to project the 2D camera data to the 3D vehicle space. This process will introduce the linearization of the h(x) function via first order taylor espansion. This is the core of the Extended Kalman Filter approach.\n",
    "\n",
    "In the current 3D case, where multivariate measurement gaussians take place, the Measurement Jacobian Hj(x) must be calculated.\n",
    "\n",
    "The Camera FOV will be narrower than the Lidar one, but as we can see below, the RMSE results improved after introducing the camera data (as we expected)\n",
    "\n",
    "<img src=\"img/writeup-final/step4-rmse.png\"/>\n",
    "\n",
    "Below, an example on how the camera data helped dealing with the ghost tracks:\n",
    "\n",
    "Ghost tracks without camera data:\n",
    "\n",
    "<img src=\"img/writeup-final/ghost-lidar.png\"/>\n",
    "\n",
    "\n",
    "After introducing camera data, the faulty detection performed by lidar is not tracked:\n",
    "\n",
    "<img src=\"img/writeup-final/ghost-camera.png\"/>\n",
    "\n",
    "\n",
    "## Writeup questions\n",
    "\n",
    "#### Which part of the project was most difficult for you to complete, and why?\n",
    "Following the exercises in class covered the majority of the steps in this final project. However, I spent most of the debugging time trying to define the association and camera model in the correct way.\n",
    "\n",
    "#### Do you see any benefits in camera-lidar fusion compared to lidar-only tracking (in theory and in your concrete results)?\n",
    "The beauty of the sensor fusion approach is it's capacity of getting the best from each sensor used. Camera and Lidar have their own advantages and disadvantages, but combination of different data sources will always improve the robustness and realibility of the system, performing a second check on false positives and false negatives, which in the Lidar realm can be caused by very light-absorbent or reflective objects.\n",
    "\n",
    "\n",
    "#### Which challenges will a sensor fusion system face in real-life scenarios? Did you see any of these challenges in the project?\n",
    "Using a state-space approach always implies creating a model of the real world scenario: the more precise the model the more accurate will be our output. In the project case, we modeled our vehicles as 'linear velocity' objects, expecting from them a modest variance in breaking/acceleration in the measurement covariance matrix. As simple as it may sound, the model will not be able to effectively predict the complex behaviour of the cars in a real urban environment (let's note that we also considered only vehicles detections, without tracking pedestrians, cyclists and so on). Considering a different model will sure increase the robustness of our tracking system.\n",
    "\n",
    "#### Can you think of ways to improve your tracking results in the future?\n",
    "There are many ways to improve our results, as also suggested in the project page:\n",
    "\n",
    "- Parameters finetuning: e.g. we could apply the standard deviation values for lidar, which can be obtained from the 3D object detection in the midterm project, to parameters in the system noise Q\n",
    "- Model choice: a more specific model (ex: bycicle non linear model) will be able to describe the dynamics of our vehicles in a more effective way.\n",
    "- Data association algorithm: our results should improve applying a more sophisticated association algorithm, like GNN and JPDA\n",
    "- Better object detection: improving the object detection performance will let our system be less prone to misclassification errors.\n",
    "- Introducing object's width, length, heigth to the Kalman Filter\n",
    "- Varying dt: varying dt could give more accurate predictions than the costant dt model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a6438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
